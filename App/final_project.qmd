---
Collaborators: "Charisma Lambert and Lizzy Diaz"
format: html
---
# Link to Data: https://drive.google.com/drive/folders/1ntjCQQKccntvuSwcqXkeLCpRBx7s2-Xz?usp=sharing

# Install Packages for Document
```{python}
import pandas as pd
import os
import csv
import altair as alt
import numpy as np
import warnings
import geopandas as gpd
from shapely import wkt
import matplotlib.pyplot as plt
import plotly.express as px
import random
warnings.filterwarnings("ignore")
```

# Import raw data
```{python, cache = True}
#raw_data_path = r"C:\Users\User\OneDrive - The University of Chicago\4_DAP-2\Final Project Data"
raw_data_path = r"/Users/charismalambert/Downloads" 

crashes = pd.read_csv(os.path.join(raw_data_path, "traffic_crashes_crashes.csv"))
vehicles = pd.read_csv(os.path.join(raw_data_path, "traffic_crashes_vehicles.csv"))
people = pd.read_csv(os.path.join(raw_data_path, "traffic_crashes_people.csv"))
```

# Keep data from 2016-present 
```{python}
dfs = [crashes, vehicles, people]

# Convert the CRASH_DATE column to datetime and filter for 2016-present
for i in range(len(dfs)):
    dfs[i]['CRASH_DATE'] = pd.to_datetime(dfs[i]['CRASH_DATE'])
    dfs[i] = dfs[i][dfs[i]['CRASH_DATE'].dt.year >= 2016]
    dfs[i]['YEAR'] = dfs[i]['CRASH_DATE'].dt.year

```


```{python}
# Function to count distinct CRASH_RECORD_ID per year
def count_distinct_crash_record_id(df):
    return df.groupby('YEAR')['CRASH_RECORD_ID'].nunique()
# Print distinct counts per year for each DataFrame
for i, df in enumerate(dfs, 1):
    print(f"DataFrame {i} distinct counts per year:\n", count_distinct_crash_record_id(df), "\n")
```

# Remove observations where location data is unavailable 
```{python}
crashes_location = crashes.dropna(subset=['LOCATION'])
```

# Remove columns we won't use 
```{python}
crashes_location = crashes_location.drop(columns=['TRAFFIC_CONTROL_DEVICE', 
'DEVICE_CONDITION','INTERSECTION_RELATED_I','NOT_RIGHT_OF_WAY_I','STREET_NO',
'STREET_DIRECTION', 'STREET_NAME', 'BEAT_OF_OCCURRENCE',
'PHOTOS_TAKEN_I', 'STATEMENTS_TAKEN_I','WORK_ZONE_I',
'WORK_ZONE_TYPE', 'WORKERS_PRESENT_I'])

people = people.drop(columns=['SEAT_NO', 'CITY', 'STATE', 'ZIPCODE',
'DRIVERS_LICENSE_STATE', 'DRIVERS_LICENSE_CLASS', 'SAFETY_EQUIPMENT',
'HOSPITAL','EMS_AGENCY', 'EMS_RUN_NO'])
```

Remove observations in people for which there is no corresponding 'crash_record_id' in crashes
do this by merging people and crashes_location

There are 161 observations where the dates for a given crash record ID do not match across the crashes and people datasets. If the mismatch in dates is greater than 7 days, I remove that observation from crashes_people

# Clean DF
```{python}
crashes_people = pd.merge(people, crashes_location, on='CRASH_RECORD_ID', how='inner')

# Check merge worked by matching crash_date_x with crash_date_y
crashes_people['check'] = crashes_people['CRASH_DATE_x'] == crashes_people['CRASH_DATE_y']
crashes_people.shape[0] - crashes_people['check'].sum()

# View observations where dates don't match 
mismatched_observations = crashes_people[crashes_people['check'] == False]
mismatched_observations['difference'] = mismatched_observations['CRASH_DATE_x'] - mismatched_observations['CRASH_DATE_y']
mismatched_observations['difference']= mismatched_observations['difference'].dt.days

# find any observations where the absolute value of the difference is greater than 7, identify those in the main dataset, and remove them
mismatched_observations['difference'] = mismatched_observations['difference'].abs()
mismatched_observations_over_7_days = mismatched_observations[mismatched_observations['difference'] > 7]

crashes_people = crashes_people[~crashes_people['CRASH_RECORD_ID'].isin(mismatched_observations_over_7_days['CRASH_RECORD_ID'])]
```

# Convert to same coordinate reference system
```{python}
# # Convert location column to geoseries
crashes_people['geometry'] = crashes_people['LOCATION'].apply(wkt.loads)
crashes_gdf = gpd.GeoDataFrame(crashes_people, geometry='geometry')

# reset CRS to IL
crashes_gdf.set_crs(epsg=4326, inplace=True)
crashes_gdf = crashes_gdf.to_crs(epsg=3435)

# Load the Chicago ward boundaries geojson
ward_boundaries = gpd.read_file(os.path.join(raw_data_path, 'ward_boundaries.geojson'))
ward_boundaries = ward_boundaries.to_crs(epsg=3435)
```

# Keep only 2020-2024, and make year column. And drop unnecessary columns. 
```{python}
# Keep only crash_date_y (from crashes dataset)
crashes_gdf = crashes_gdf.drop(columns=['CRASH_DATE_x'])
crashes_gdf = crashes_gdf.rename(columns={'CRASH_DATE_y':'CRASH_DATE'})
crashes_gdf = crashes_gdf.drop(columns=['check'])

crashes_gdf['YEAR'] = crashes_gdf['CRASH_DATE'].dt.year
crashes_gdf = crashes_gdf[crashes_gdf['YEAR'] >= 2021]
```

# Create SEASON column 
```{python}
# add SEASON column to create heatmap of accidents by season 
def fill_season(month):
    if month in [12, 1, 2]: 
        return "Winter"
    elif month in [3, 4, 5]:
        return "Spring"
    elif month in [6, 7, 8]:
        return "Summer"
    else: 
        return "Fall"

crashes_gdf["SEASON"] = crashes_gdf["CRASH_MONTH"].apply(fill_season)
```

# Save data to csv for app
```{python}
#crashes_gdf.to_csv('crashes_gdf.csv', index=False)
```

# Attempt Basic Heatmap before App integration 
```{python}
# Basic Code before app 
tracking_by_season = crashes_gdf[crashes_gdf["YEAR"] == 2024].groupby(["SEASON", "CRASH_HOUR"]).size().reset_index(name = "count")

heatmap = alt.Chart(tracking_by_season).mark_rect().encode(
    x = "CRASH_HOUR:O",
    y = "SEASON:O",
    color = "count:Q", 
).properties(
    width = 600,
    height = 300
)

heatmap
```

# Code to add to the App:
```{python}
# Dynamic for App: 
# Include drop down for year (user modify) and crash_hour (constant)
# Year range: 2021 - 2024

def accidents_by_season(year):
    tracking_by_season = crashes_gdf[crashes_gdf["YEAR"] == year].groupby(["SEASON", "CRASH_HOUR"]).size().reset_index(name = "count")

    heatmap = alt.Chart(tracking_by_season).mark_rect().encode(
        x = "CRASH_HOUR:O",
        y = "SEASON:O",
        color = "count:Q", 
    ).properties(
        title=f"Number of Accidents during each Season in {year}",
        width = 600,
        height = 300
    ).configure_title(
        fontSize = 18
    )

    heatmap.show()

accidents_by_season(2021)
```

# Recode categorical column values to numbers
```{python}
# convert IV column values to numerical values to make them regression friendly and store in new df 
# recode SEX as M: 1, F: 0, X and Nan = 2: 
crashes_gdf["SEX_R"] = crashes_gdf["SEX"].replace({"M": 1, "F": 0, "X": 2, None: -1}).astype("float64")

# recode SEASON as Winter: 1, Spring: 2, Summer: 3, Fall: 4
crashes_gdf["SEASON_R"] = crashes_gdf["SEASON"].replace({"Winter": 1, "Spring": 2, "Summer": 3, "Fall": 4}).astype("float64")


# recode PERSON_TYPE as Driver:0, Non-Driver(Passenger, Pedestrian,  Bicycle, Non-motor vehicle, Non-contact_vehicl): 1
crashes_gdf["PERSON_TYPE_R"] = crashes_gdf["PERSON_TYPE"].replace({
    'DRIVER':1,
    'PASSENGER': 2,
    'PEDESTRIAN': 3, 
    'BICYCLE': 4, 
    'NON-MOTOR VEHICLE': 5, 
    'NON-CONTACT VEHICLE': 6
}).astype("float64")

```


```{python}
# convert DV column values to numerical values to make them regression friendly

# recode AIRBAG_DEPLOYED as Deploed, Combo: 1, Delpoyed, Front: 1, Deployed, Side: 1, Deployed Other:1, Did Not Deploy: 0, Not Applicable: 0, Deployment Unknown: 0
crashes_gdf["AIRBAG_DEPLOYED_R"] = crashes_gdf["AIRBAG_DEPLOYED"].replace({
    'DEPLOYED, COMBINATION': 1,
    'DID NOT DEPLOY': 2,
    'NOT APPLICABLE': 3,
    'DEPLOYMENT UNKNOWN': 4,
    'DEPLOYED, FRONT': 5,
    'DEPLOYED, SIDE': 6,
    'DEPLOYED OTHER (KNEE, AIR, BELT, ETC.)': 7,
    None : -1
}).astype("float64")


# recode INJURY_CLASSIFICATION as Injured( 'NONINCAPACITATING INJURY': 1, 'REPORTED, NOT EVIDENT': 1,  'INCAPACITATING INJURY': 1, 'FATAL': 1) and Non-Injured ('NO INDICATION OF INJURY': 0, nan:0)
crashes_gdf["INJURY_CLASSIFICATION_R"] = crashes_gdf["INJURY_CLASSIFICATION"].replace({
    'NO INDICATION OF INJURY': 0,
    'NONINCAPACITATING INJURY': 1 ,
    'REPORTED, NOT EVIDENT': 2,
    'INCAPACITATING INJURY': 3 ,
    'FATAL': 4 ,
    None: -1
}).astype("float64")

# recode EJECTION as Ejected('TOTALLY EJECTED': 1, 'PARTIALLY EJECTED': 1) or Not-Ejeccted ('TRAPPED/EXTRICATED': 0, 'NONE': 0, 'UNKNOWN': 0,  nan: 0)
crashes_gdf["EJECTION_R"] = crashes_gdf["EJECTION"].replace({
    'TOTALLY EJECTED':  1,
    'TRAPPED/EXTRICATED' : 2,
    'PARTIALLY EJECTED': 3 ,
    'NONE': 0,
    'UNKNOWN': 0 ,
    None: - 1
}).astype("float64")


regression_df = crashes_gdf.dropna(subset = ["AGE"])
regression_df
```

# Save to new df 
```{python}
def reduce_data(data):
    # List of years in the dataset
    years = data["YEAR"].unique()
    
    # This will hold the reduced DataFrame
    reduced_df_list = []
    
    for year in years:
        # Filter the data for the current year
        year_data = data[data["YEAR"] == year]
        
        # Perform stratified sampling, keeping exactly 1000 rows for each year
        # Group by 'SEX_R' and 'PERSON_TYPE_R', and sample within each group
        stratified_sample = year_data.groupby(['SEX_R', 'PERSON_TYPE_R'], group_keys=False).apply(
            lambda x: x.sample(frac=1, random_state=42)
        )
        
        # Now sample 1000 rows from the stratified dataset
        if len(stratified_sample) > 1000:
            stratified_sample = stratified_sample.sample(n=1000, random_state=42)
        
        reduced_df_list.append(stratified_sample)
    
    # Concatenate all sampled dataframes into one
    reduced_df = pd.concat(reduced_df_list)
    
    return reduced_df

# Apply the function to reduce the size of the dataframe
regressions_df_reduced = reduce_data(regression_df)

regression_df.to_csv('regression_df.csv', index=False)

# Citation: Initially, I did a random sample of 1000 rows per year but I did not norm that my 
# categorical variables were the same and when I ran the rest of my code, I noticed that SEX was only male, etc. I ran a query in ChatGPT on how to keep the proportion of categorical variables the same in my sample, as is in the original dataset. The query returned lines "stratified_sameple".. and on. 
```

# Basic regressions before adding to App:
IV: AGE, SEX_R, SEASON_R, PERSON TYPE_R
DV: AIRBAG_DEPLOYED_R, INJURY_CLASSIFICATION_R, EJECTION_R
```{python}
def variable_relationships(IV, DV):
    if IV == "AGE":
        fig = px.box(regression_df, 
            x = DV, 
            y = IV,
            color = DV,
            title = f"{IV} vs {DV} for Traffic Crashes in 2024",
            labels = {IV: IV, DV: DV})
    else: 
        fig = px.histogram(regression_df,
            x = IV,
            y = DV,
            barmode = 'group',
            title = f"{IV} vs {DV} for Traffic Crashes in 2024",
            labels = {IV: IV, DV: DV})
    fig.update_layout(legend_title_text = DV)
    fig.show()

variable_relationships("SEX", "EJECTION_R")
```

# Code to add to the App:
```{python}
# Drop down for IV, DV, YEAR
# IV's = Age, Sex, Season, Person_Type
# DV's = Injury_Classification, Airbag_Deployed, Ejection
# Year

# Based on what is chosen, produce regression stats that are user friendly

def variable_relationships(IV, DV, year):
    use_this_df = regression_df[regression_df["YEAR"] == year]
    if IV == "AGE":
        fig = px.box(use_this_df, 
            x = DV, 
            y = IV,
            color = DV,
            title = f"{IV} vs {DV} for Traffic Crashes in {year}",
            labels = {IV: IV, DV: DV})
    else: 
        fig = px.histogram(use_this_df,
            x = IV,
            y = DV,
            barmode = 'group',
            title = f"{IV} vs {DV} for Traffic Crashes in 2024",
            labels = {IV: IV, DV: DV})
    fig.update_layout(legend_title_text = DV)
    fig.show()

variable_relationships("PERSON_TYPE", "EJECTION", 2023)
```
