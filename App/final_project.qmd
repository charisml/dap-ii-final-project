

```{python}
import pandas as pd
import os
import csv
import altair as alt
import numpy as np
import warnings
warnings.filterwarnings("ignore")
```


```{python}
#base_path = r"/Users/charismalambert/Documents/GitHub/dap-ii-final-project/Data"
base_path = r"C:\Users\User\Documents\GitHub\dap-ii-final-project\Data"
  
vehicles_path = os.path.join(base_path, "vehicles_crash_instances.csv")
vehicles_df = pd.read_csv(vehicles_path)

crashes_path = os.path.join(base_path, "crashes_crash_instances.csv")
crashes_df = pd.read_csv(crashes_path)

people_path = os.path.join(base_path, "people_crash_instances.csv")
people_df = pd.read_csv(people_path)

crash_by_type_path = os.path.join(base_path, "crash_by_type.csv")
crash_by_type = pd.read_csv(crash_by_type_path)
```

Read in raw data (so I can use all columns), and keep only 10,000 rows 

```{python}
raw_data_path = r"C:\Users\User\OneDrive - The University of Chicago\4_DAP-2\Final Project Data"

crashes = pd.read_csv(os.path.join(raw_data_path, "traffic_crashes_crashes.csv"))
vehicles = pd.read_csv(os.path.join(raw_data_path, "traffic_crashes_vehicles.csv"))
people = pd.read_csv(os.path.join(raw_data_path, "traffic_crashes_people.csv"))

dfs = [crashes, vehicles, people]
# keep 10000 rows of data
for df in dfs:
    df = df[:10000]
crashes = crashes[:10000]
vehicles = vehicles[:10000]
people = people[:10000]
```

Data cleaning and exploration

```{python}
for df in dfs: 
    df['CRASH_DATE'] = pd.to_datetime(df['CRASH_DATE'])
    df['YEAR'] = df['CRASH_DATE'].dt.year

# Function to count distinct CRASH_RECORD_ID per year
def count_distinct_crash_record_id(df):
    return df.groupby('YEAR')['CRASH_RECORD_ID'].nunique()

# Print distinct counts per year for each DataFrame
for i, df in enumerate(dfs, 1):
    print(f"DataFrame {i} distinct counts per year:\n", count_distinct_crash_record_id(df), "\n")

```

Note for Charisma: The problem with taking only the first 10,000 rows is that we may end up with incomplete cases - where there is an observation in crashes for 2015, that may not appear in the other 2 datasets because it is cut off, or vice versa. Or, we may not be counting all crashes from 2018, 2019, 2023, etc. So instead of keeping the first 10,000 rows, I'm going to keep all data from 2016 to present. 


```{python}
crashes = pd.read_csv(os.path.join(raw_data_path, "traffic_crashes_crashes.csv"))
vehicles = pd.read_csv(os.path.join(raw_data_path, "traffic_crashes_vehicles.csv"))
people = pd.read_csv(os.path.join(raw_data_path, "traffic_crashes_people.csv"))
```

Keep data from 2016-present 
```{python}
dfs = [crashes, vehicles, people]

# Convert the CRASH_DATE column to datetime and filter for 2016-present
for i in range(len(dfs)):
    dfs[i]['CRASH_DATE'] = pd.to_datetime(dfs[i]['CRASH_DATE'])
    dfs[i] = dfs[i][dfs[i]['CRASH_DATE'].dt.year >= 2016]
    dfs[i]['YEAR'] = dfs[i]['CRASH_DATE'].dt.year

```


```{python}
# Function to count distinct CRASH_RECORD_ID per year
def count_distinct_crash_record_id(df):
    return df.groupby('YEAR')['CRASH_RECORD_ID'].nunique()
# Print distinct counts per year for each DataFrame
for i, df in enumerate(dfs, 1):
    print(f"DataFrame {i} distinct counts per year:\n", count_distinct_crash_record_id(df), "\n")
```

Remove observations where location data is unavailable 
```{python}
crashes_location = crashes.dropna(subset=['LOCATION'])
```

Remove columns I won't use 
```{python}
crashes_location = crashes_location.drop(columns=['TRAFFIC_CONTROL_DEVICE', 
'DEVICE_CONDITION','INTERSECTION_RELATED_I','NOT_RIGHT_OF_WAY_I','STREET_NO',
'STREET_DIRECTION', 'STREET_NAME', 'BEAT_OF_OCCURRENCE',
'PHOTOS_TAKEN_I', 'STATEMENTS_TAKEN_I','WORK_ZONE_I',
'WORK_ZONE_TYPE', 'WORKERS_PRESENT_I'])

people = people.drop(columns=['SEAT_NO', 'CITY', 'STATE', 'ZIPCODE',
'DRIVERS_LICENSE_STATE', 'DRIVERS_LICENSE_CLASS', 'SAFETY_EQUIPMENT',
'HOSPITAL','EMS_AGENCY', 'EMS_RUN_NO'])
```

Remove observations in people for which there is no corresponding 'crash_record_id' in crashes
do this by merging people and crashes_location
```{python}
crashes_people = pd.merge(people, crashes_location, on='CRASH_RECORD_ID', how='inner')
```

Create a basic map before starting the app
```{python}
import geopandas as gpd
import pandas as pd
from shapely import wkt
import matplotlib.pyplot as plt
```

```{python}
# # Convert location column to geoseries
crashes_people['geometry'] = crashes_people['LOCATION'].apply(wkt.loads)
crashes_gdf = gpd.GeoDataFrame(crashes_people, geometry='geometry')

# reset CRS to IL
crashes_gdf.set_crs(epsg=4326, inplace=True)
crashes_gdf = crashes_gdf.to_crs(epsg=3435)

# Load the Chicago ward boundaries geojson
ward_boundaries = gpd.read_file(os.path.join(raw_data_path, 'ward_boundaries.geojson'))
ward_boundaries = ward_boundaries.to_crs(epsg=3435)

# Plot the ward boundaries and crash points
fig, ax = plt.subplots(figsize=(10, 10))
ward_boundaries.plot(ax=ax, color='lightgrey', edgecolor='black')
crashes_gdf.plot(ax=ax, color='red', markersize=1, alpha=0.7)
plt.show()

```

Dataset is still too big. keep only 2018-2023, and make year column

```{python}
crashes_gdf['YEAR'] = crashes_gdf['CRASH_DATE'].dt.year
crashes_gdf = crashes_gdf[crashes_gdf['YEAR'] >= 2018 &
crashes_gdf['YEAR'] <= 2023]
```

Save data to csv for app
```{python}
crashes_gdf.to_csv('crashes_gdf.csv', index=False)
```
